## 목표

- BMI 지수 = (몸무게 / 키) * 키
- BMI를 이용하여 비만도를 측정
- 알고리즘 SVM을 이용하여 BMI를 학습, 비만도 판정

## 데이터 획득

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
df = pd.read_csv('3_bmi.csv')
```

## 데이터 준비

```python
# 데이터 확인
df.shape, df.columns, df['label'].unique()
df.dtypes
df['height'].max(), df['height'].min(), df['weight'].max(), df['weight'].min()

# 데이터 정규화 -> 무게, 키의 최대치를 기준으로 처리
# 처리된 값을 다시 해당 컬럼에 대입
df['height'] = df['height'] / df['height'].max()
df['weight'] = df['weight'] / df['weight'].max()

# label은 종속변수. 분류를 벡터화해서 처리
# 케이스 총 3개
# thin->[1, 0, 0] / normal->[0, 1, 0] / fat->[0, 0, 1]
tmp = {'thin':np.array([1,0,0]), 'normal':np.array([0,1,0]), 'fat':np.array([0,0,1])}
df['label'] = df['label'].apply(lambda x : tmp[x])

X_train, X_test, y_train, y_test = train_test_split(df[['height', 'weight']], df['label'], test_size=0.25, random_state=0)
```

## 데이터 분석

- 생략

## 머신러닝 모델링(딥러닝, 텐서플로우 사용)

- 퍼셉트론
- 각각의 출력값에 대해 가중치 계산, 바이어스(편향, 조정한 값)를 더해서 softmax라는 함수로 적용
- 해당 이론을 수식으로 정리하여 함수, 데이터 플로우 그래프로 표현
- y = softmax(Wx + b)

![](C:\Users\admin\Documents\GitHub\pengsoo\dl\data\sm0.png)

![sm1](C:\Users\admin\Documents\GitHub\pengsoo\dl\data\sm1.png)

![sm2](C:\Users\admin\Documents\GitHub\pengsoo\dl\data\sm2.png)

![sm3](C:\Users\admin\Documents\GitHub\pengsoo\dl\data\sm3.png)



### 데이터 플로우 그래프 준비

```python
import tensorflow as tf
%tensorflow_version 1.x

# input(x): 키, 몸무게
x = tf.placeholder(tf.float32, [None,2])

# 가중치: shape 고려
W = tf.Variable(tf.zeros([2,3]))

# b(bias): 편향값
b = tf.Variable(tf.zeros([3]))

# 데이터 플로우 그래프: y = softmax(Wx+b)
# x: matrix, 2차 행렬, [none, 2]
# 행렬의 곱 (none, 2) * (2, 3) -> (none, 3)
y = tf.nn.softmax(tf.matmul(x,W)+b)

f'{x.shape} X {W.shape} + {b.shape} = {y.shape}'
```



### 학습에 필요한 데이터 플로우 그래프 추가

- 좋은 모델을 만들기 위한 장치
  - 비용(cost), 손실(loss) 등의 값들이 원하는 결과에서 얼마나 떨어져 있는가를 판단/ 이 값들을 줄이는 방향으로 전개
  - 이런 처리를 위해서 '크로스 엔트로피'와 같은 알고리즘 적용
  - 정보 이론 분야의 정보 압축 알고리즘으로 고안되었음

![](C:\Users\admin\Documents\GitHub\pengsoo\dl\data\sm4.png)

```python
# y_: 정답 label
# y : 예측 label
# y_ = [1,0,0], [0,1,0], [0,0,1] => [None, 3]
y_ = tf.placeholder(tf.float32, [None,3])

# 크로스 엔트로피를 구성
cross_entropy = -tf.reduce_sum(y_ * tf.log(y))

# 크로스 엔트로피가 최소화 되도록 작업을 진행
# 경사 하강법(Gradient descent algorithm)으로 수행
# 텐서플로우가 변수의 비용을 줄이는 방향으로 조금씩 이동시켜서 연산 수행 => 비용과 손실, 오차율이 최소가 되게끔
# 하이퍼파라미터는 일단 0.01 적용
optimizer = tf.train.GradientDescentOptimizer(0.01)

# 훈련 정의
train = optimizer.minimize(cross_entropy)

# 정답률 예측 정의
# tf.argmax(y,1): 모델이 각 데이터별로 적합하다고 판단하는 레이블 값을 취함
predict = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))

# 정확도
accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))
```



### 세션 가동 및 실행

- 데이터를 증가시키면서 반복 학습 방법
  1. 0\~100, 0\~200, ... 데이터의 전체 양 증가
  2. 0\~100, 100\~200, ... 데이터의 양 고정, 횟수만 증가
  
  - 횟수 증가

```python
with tf.Session() as sess:
  # 1. 변수(조건) 초기화
  sess.run(tf.global_variables_initializer())
  
  # 2. 훈련 데이터를 나누기 위한 기준값 정의
  # 2-1. 훈련 한번 수행시 데이터 크기
  TRAIN_TERM = 100
  # 2-2. 훈련 진행 횟수
  train_count = int(X_train.shape[0] / TRAIN_TERM)
  
  # 3. 반복 학습 수행
  for step in range(train_count):
    # 3-1. 훈련데이터에서 데이터를 순차적으로 추출
    offset = step * TRAIN_TERM
    # 3-2. 학습
    # 학습 데이터 준비: 플레이스홀더에 값 채우기(테스트 데이터와 학습 데이터의 피쳐 순서 맞춰야 함)
    fd = {x:X_train[offset : offset + TRAIN_TERM], y_:list(y_train[offset : offset + TRAIN_TERM])}
    # 학습 수행
    sess.run(train, feed_dict=fd)
    # 중간 확인
    if step%10 == 0:
      # 크로스 엔트로피 값 획득
      cross_en = sess.run(cross_entropy, feed_dict=fd)
      # 정확도 획득: 테스트 데이터와 데스트 데이터의 답을 주입
      acc = sess.run(accuracy, feed_dict={x:X_test, y_:list(y_test)})
      print(f'step={step}, cross={cross_en}, acc={acc}')
  acc = sess.run(accuracy, feed_dict={x:X_test, y_:list(y_test)})
  print(f'마지막 횟수의 정확도={acc}')
```

```PYTHON
df = pd.read_csv('3_bmi.csv')
df['height'] = df['height'] / df['height'].max()
df['weight'] = df['weight'] / df['weight'].max()
tmp = {'thin':np.array([1,0,0]), 'normal':np.array([0,1,0]), 'fat':np.array([0,0,1])}
df['label'] = df['label'].apply(lambda x : tmp[x])
X_train, X_test, y_train, y_test = train_test_split(df[['height', 'weight']], df['label'], test_size=0.25, random_state=0)
x = tf.placeholder(tf.float32, [None,2])
W = tf.Variable(tf.zeros([2,3]))
b = tf.Variable(tf.zeros([3]))
y = tf.nn.softmax(tf.matmul(x,W)+b)
y_ = tf.placeholder(tf.float32, [None,3])
cross_entropy = -tf.reduce_sum(y_ * tf.log(y))
optimizer = tf.train.GradientDescentOptimizer(0.01)
train = optimizer.minimize(cross_entropy)
predict = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(predict, tf.float32))
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())
  TRAIN_TERM = 100
  train_count = int(X_train.shape[0] / TRAIN_TERM)
  STEP_SEGMENT = 2*2*2*2
  for step in range(train_count*STEP_SEGMENT):
    offset = int(step * TRAIN_TERM / STEP_SEGMENT)
    fd = {x:X_train[offset : offset + TRAIN_TERM], y_:list(y_train[offset : offset + TRAIN_TERM])}
    sess.run(train, feed_dict=fd)
    if step%STEP_SEGMENT == 0:
      cross_en = sess.run(cross_entropy, feed_dict=fd)
      acc = sess.run(accuracy, feed_dict={x:X_test, y_:list(y_test)})
      print(f'step={step}, cross={cross_en}, acc={acc}')
  acc = sess.run(accuracy, feed_dict={x:X_test, y_:list(y_test)})
  print(f'마지막 횟수의 정확도={acc}')
```



## 텐서보드

- 목적: 텐서보드를 이용하여 그래프(데이터 흐름)를 시각적으로 확인
- 방법(코드적으로 조금 다름)
  1. 로컬 PC에서 수행
  2. colab에서 수행
  
  * 데이터가 누적되기 때문에 클리어 후 처리해야 정확하게 나옴

```python
# 텐서 데이터 플로우 구현
x             = tf.placeholder( tf.float32, [None,2] )
y_            = tf.placeholder( tf.float32, [None, 3] )        # 정답

with tf.name_scope('interface') as scope:
  W             = tf.Variable( tf.zeros([2, 3]) ) 
  b             = tf.Variable( tf.zeros([3]) )
  with tf.name_scope('activation') as scope:
	  y  = tf.nn.softmax(tf.matmul(x,W) + b)      # 간단한 입력->출력층구성

with tf.name_scope('loss') as scope:
	cross_entropy = -tf.reduce_sum( y_ * tf.log(y) )               # 크로스엔트로피

with tf.name_scope('train') as scope:
	optimazer     = tf.train.GradientDescentOptimizer(0.01)        # 경사하강법
	train         = optimazer.minimize( cross_entropy )            # 훈련

with tf.name_scope('accuracy') as scope:
  predict       = tf.equal( tf.argmax(y, 1), tf.argmax(y_, 1) )  # 예측
  accuracy      = tf.reduce_mean( tf.cast(predict, tf.float32) ) # 평가

# 텐서보드를 위해서 추가된 코드(코렙에서 사용하는 스타일) --------------------
from tensorboardcolab import *
import shutil, os
# 디렉토리 및 그 이하 파일가지 삭제, 에러나면 무시
shutil.rmtree('./Graph', ignore_errors=True) 
# 디렉토리 생성
os.mkdir('./Graph')
# 기존의 tf에 만들어진 요소들 초기화 처리(에러가 날수도 있다)
# tf.reset_default_graph()
# 텐서보드 객체 생성
tbc = TensorBoardColab()
# ---------------------------------------------------------------------------

# 실행을 통한 학습,예측처리
with tf.Session() as sess:
  sess.run( tf.global_variables_initializer() )
  TRAIN_TERM = 100  
  train_count = int(X_train.shape[0] / TRAIN_TERM)  
  STEP_SEGMENT = 2*2*2*2
  for step in range(train_count*STEP_SEGMENT): # step:(0~149)    
    offset = int(step*TRAIN_TERM / STEP_SEGMENT)
    fd = {x:X_train[offset : offset + TRAIN_TERM], y_:list(y_train[offset : offset + TRAIN_TERM])}
    sess.run( train, feed_dict=fd )
    if step%STEP_SEGMENT == 0:
      cross_en = sess.run( cross_entropy, feed_dict=fd )
      acc      = sess.run( accuracy, feed_dict={x:X_test, y_:list(y_test)} )
      print(f'step={step}, cross={cross_en}, acc={acc}')
  acc = sess.run( accuracy, feed_dict={x:X_test, y_:list(y_test)} )
  print( f'정확도: {acc}')
  # 코렙 텐서보드의 물리적위치에 그래프를 기록한다 ----------------------
  writer = tbc.get_writer()
  writer.add_graph( sess.graph ) # 그래프 추가=
  writer.flush() # 강제로 전송
  # ---------------------------------------------------------------------

# 텐서보드를 닫는다 
tbc.close()
```



## 시스템 통합

- 산출물 skip
