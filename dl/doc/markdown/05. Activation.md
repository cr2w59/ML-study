# 활성화 함수

- 선형 방식(시스템)을 망에 적용하면 깊게 표현되지 않고 쉽게 예측 가능 -> 딥하게 레이어를 구축하고 싶어도 이미 예측이 쉽게 되서 딥러닝을 구성할수 없다.
- 딥러닝 환경에서 망이 깊어지면(레이어가 많아지면) > 은닉층이 많아지게 되고 > 매개변수가 줄어들고, 연산수가 줄어들어서 > 정확도의 향상을 가져온다
- 사용 이유 : data를 비선형으로 바꾸기 위해서

| 종류 | 이름       | 특징                                                  |
| ---- | ---------- | ----------------------------------------------------- |
| 1    | sigmoid    | Input: 실수, Output: 0~1                              |
| 2    | Tanh       | Input: 실수, Output: -1~1                             |
| 3    | ReLu       | **Input: 음수, Output=0 / Input: 양수, Output=Input** |
| 4    | Leaky ReLu | Input: 음수, Output: 0.01과 같은 작은 수              |
| 5    | PReLu      | Leaky ReLu와 유사함. 음수일 때 기울기 조정            |
| 6    | ELU        | y축에서 -1로 이동해 0으로 근사하게끔 미분처리         |
| 7    | Maxout     | ReLu와 Leaky ReLu의 일반화 모델                       |



## Sigmoid

- 실수를 취하고, 0과 1 사이의 숫자를 출력
- 음수는 클수록 0에 수렴, 양수는 클수록 1에 수렴
- 오랫동안 사용되어 왔으나, 큰 단점이 있어서 지금은 많이 사용되지 않음
- 출력값이 1이나 0에 가까워지면, 기울기가 0이 되는 문제
- 가중치가 미반영되어서 학습이 안됨 -> 죽은 뉴런

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/5.sigmoid.png?raw=true)



## Tanh

- -1\~1로 변환
- 단점은 시그모이드와 동일
- 시그모이드 대비 폭이 넓어짐. 0을 중심으로 위아래로 변화값, 변동폭이 더 큼
- 시그모이드보다는 많이 사용

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_2.png?raw=true)

<img src="https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_3.png?raw=true" style="zoom: 67%;" />



## ReLu

- ReLu 계열을 가장 많이 사용
- 음수면 0, 양수면 자기자신을 값으로 취한다
- 장점
  1. 옵티마이저(경사하강법 등)가 더 빠르게 가중치를 찾도록 도움
  2. SGD(확률적 경사하강법)에서도 더 빠르게 수렴하도록 지원
  3. 연산 비용 저렴, 임계값을 가짐
- 단점
  1. 역전파 진행시 기울기가 큰 값을 가진 경우 응답 없음-> 죽은 뉴런(학습률 저하 요소)
- 이미지 인식대회에서 우승한 알고리즘 AlexNet에서 ReLu 사용해 tanh보다 6배의 성능 향상을 가져왔다는 논문

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_4.png?raw=true)

<img src="https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_5.png?raw=true" style="zoom:67%;" />



## Leaky ReLu

- 만약 ReLu 사용시 0으로 수렴되면 학습이 진행되지 않는 문제를 해결하기 위해 제안
- ReLu의 변형
- 입력값이 음수면 0.01을 적용하여 수렴
- 간혹 ReLu보다 나은 성과를 내기도 함

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_6.png?raw=true)

<img src="https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_7.png?raw=true" style="zoom: 33%;" />



## PReLu

- Parametric ReLu
- 음수값에 알파값을 적용해 수렴

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_8.png?raw=true)



## ELU

- ReLu의 장점을 포함
- 죽은 뉴런을 해결하기 위해 음수대에서 0으로 수렴되는 위치를 x값 0에서 -5로 
- 미분 계산에 대한 비용

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_9.png?raw=true)

<img src="https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_10.png?raw=true" style="zoom: 67%;" />



## Maxout

- ReLu와 Leaky ReLu를 일반화 함
- 장점은 모두 취하고, 죽은 뉴런 문제도 해결
- 파라미터가 2배로 증가하여 전체 파라미터 수가 증가하는 단점 존재

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_11.png?raw=true)

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/3_12.png?raw=true)