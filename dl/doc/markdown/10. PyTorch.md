# **PyTorch**

- https://pytorch.org/
- python/C++/Java을 위한 오픈소스 라이브러리
- GPU 지원은 NDIVIA만 가능(CUDA만 사용가능)
- Lua, C++ 등으로 개발됨
- 딥러닝 프레임워크(엔진)
- 장점
  1. 복잡하고, 계산량이 큰 그래프를 쉽게 구성
  2. 계산 그래프를 활용하면, 미/적분 간단하게 처리
  3. GPU 연산을 활용해 빠른 처리 가능

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/PyTorch install.PNG?raw=true)

## 특징

- numpy 기반으로 tensor 연산을 GPU로 수행 가능
- 자동 미분 시스템을 이용하여 모델을 손쉽게 구성
- 같이 활용되는 python 패키지
  * scipy
  * numpy
  * cython : 파이썬의 빠른 생산성은 유지하면서, 외부 C 라이브러리와 연동 되어 실행속도 향상
- 학습, 추론 속도가 빠르며 다루기 쉽다

## PyTorch vs. TensorFlow

- 2019년 기준, 2위-PyTorch 1위-TensorFlow
- TensorFlow
  - Define-and-Run 방식
    1. 계산 그래프, 데이터 플로우 그래프 설계
    2. 세션에서 그래프에 들어갈 데이터를 주입
    3. 세션 실행
  - 산업용
- PyTorch
  - Define-by-Run 방식
    1. 계산 그래프를 그림과 동시에 
    2. 데이터를 보관하고
    3. 바로 연산 가능함
  - 학계/연구용, 2018년 1.0 런칭 이후 산업용으로 확장 중

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
```

## 텐서

- PyTorch 기반의 딥러닝은 텐서를 사용하는 연산의 연속
- 딥러닝 구현에 있어 가장 기본의 단위

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/8.tensor.jpeg?raw=true)



### 텐서 생성

- Tensor(리스트 or 배열)
  - datatype이 int에서 float32로 자동 변환됨

```python
# 리스트로부터 텐서 생성
list_src = [ [10, 11, 12], [20, 21, 22] ]
x = torch.Tensor(list_src) # define by run이라는 취지에서 이미 텐서에 데이터가 설정되어 있음
# 복원 : 텐서 -> 리스트 추출 
x.tolist()

# ndarray로부터 텐서 생성
arr = np.array(x.tolist())
x = torch.Tensor(arr)
# 복원
x.numpy()
```

 1.  랜덤

     ```python
     # 0~1 사이로 랜덤하게 2x3행렬 생성
     # rand: *size는 가변인자->shape 설명
     torch.rand(2,3)
     
     # 0~8 사이로 랜덤하게 2x3 int행렬 생성
     torch.randint(low=0, high=8, size=(2,3))
     ```

 2.  zeros / ones / like

     ```python
     torch.zeros(2,2)
     torch.ones(3,3)
     
     # shape이 x의 shape과 동일한 행렬
     torch.zeros_like(x)
     ```

	3. gpu 이용

    ```python
    torch.zeros_like(x.cuda())
    ```

### 텐서 타입

- Data Type 확인은  *x.type()* 

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/8.torch_type.png?raw=true)

1. 실수형

   ```python
   a = torch.FloatTensor(np.array([1,2,3,4]))
   a, type(a), a.type()  #PyTorch에서 타입 확인은 x.type()
   ```

2. 정수형

   ```python
   a = torch.IntTensor(np.array([1,2,3,4]))
   a, type(a), a.type()
   ```

3. boolean형

   ```python
   a = torch.BoolTensor(np.array([True, False, 1, 0]))
   a, type(a), a.type()
   ```

   

### 텐서 조작(행렬 조작)

- slicing
- view
- transpose
- squeeze, unsqueeze
- cat, stack
- 사칙연산, dot
- sum, mean, max, argmax, min, argmin
- 논리연산

1. **Indexing, Slicing**

   * 텐서에서 특정 정보를 추출, 변경할 경우

   * 인덱싱(차원축소) or 슬라이싱을 통해 처리 가능

   * 표현 : Tensor_name[1d, 2d, 3d...]

```python
torch.manual_seed(510)  # seed 고정
x = torch.randint(low=0, high=10, size=(2,3,4))
x[0][0], x[0,0], x[0,0,0]

# 시각화 함수
# src: Tensor
def drawTensor(src):
  fig, axes = plt.subplots(len(src), 1)
  for i in range(len(src)):
    axes[i].matshow(src.numpy()[i], vmin=0, vmax=1, cmap='rainbow')
  plt.show()
```

```python
tmp = torch.ones_like(x)
drawTensor(tmp)
```

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/tensor_slicing1.png?raw=true)

```python
tmp[0,2,3]=0
drawTensor(tmp)
```

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/tensor_slicing2.png?raw=true)

```python
tmp[0, :2, 3] = 0
tmp[0,2,3] = 1
drawTensor(tmp)
```

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/tensor_slicing3.png?raw=true)

2. **view**
   - 텐서의 shape 변경(=reshape)

```python
x.shape, x.dtype, x.ndim, x
x.view(2,2,6) # x.reshape(2,2,6)와 같은 표현
# -1을 사용하면 자동으로 맞춰줌
x.view(-1, 1, 12)
```

3. **transpose**
   - 차원의 맞교환 ex. (2,3,4)->(3,2,4)

```python
x, x.shape
tmp2 = x.transpose(0,1)
tmp2, tmp2.shape
```

4. **squeeze, unsqueeze**
   - squeeze: 크기가 1인 차원을 모두 제거
   - unsqueeze: 특정 인덱스에 크기가 1인 차원을 추가

```python
x = torch.rand((2,1,3,4,1))

# 모든 차원에서 크기가 1인 차원을 제거하여 크기 줄임 -> (2,3,4)
x.squeeze(), x.squeeze().size()

# x.squeeze(n): 특정 인덱스 n이 1차면 제거
x.shape, x.squeeze(1).size(), x.squeeze(2).size(), x.squeeze(4).size()

# x.unsqueeze(n): 특정 인덱스 n에 1차원 삽입
x.unsqueeze(3).size()
```

5. **cat, stack**
   - 텐서 합치거나 쌓기 -> 경우에 따라 차원이 축소될 수도 있음
   - cat: 특정 차원을 중심으로 단순 합치기, 해당 차원의 수가 늘어난다
   - stack: 텐서를 쌓음. 하나의 차원이 추가됨

```python
# 시각화 함수 확장
def drawTensorExt(src):
  subplot_size = 1 if src.ndimension() <= 2 else src.size(0)
  fig, axes = plt.subplots(subplot_size, 1)
  if subplot_size == 1:
    axes.matshow(src.numpy(), vmin=0, vmax=1, cmap='Blues')
  else:
    for i in range(len(src)):
      axes[i].matshow(src.numpy()[i], vmin=0, vmax=1, cmap='Blues')
  plt.show()

a = torch.rand((2,3))
b = torch.rand((3,3))
ab_cat = torch.cat([a,b], dim=0) # dim=0(index) -> 1차원 기준
ab_cat, ab_cat.size()

c = torch.rand((2,2))
d = torch.rand((2,4))
cd_cat = torch.cat([c,d], dim=1) # dim=1 -> 2차원 기준
```

```python
drawTensorExt(a), drawTensorExt(b), drawTensorExt(ab_cat)
```

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/tensor_cat1.png?raw=true)

```python
drawTensorExt(c), drawTensorExt(d), drawTensorExt(cd_cat)
```

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/tensor_cat2.png?raw=true)

```python
e = torch.rand((3,5))
f = torch.rand((3,5))
# 텐서를 쌓음-> 차원을 새로 만들어야 함
ef_stack = torch.stack([e,f], dim=0)
ef_stack.size()
drawTensorExt(ef_stack)
```

![](https://github.com/cr2w59/pengsoo/blob/master/dl/doc/images/tensor_stack.png?raw=true)

6. **사칙연산, 내적(dot)**

```python
x = torch.Tensor([1,2,3])
y = torch.Tensor([4,1,3])

# 덧셈
x + y, torch.add(x,y)
# 뺄셈
x - y, torch.sub(x,y)
# 곱셈
x * y, torch.mul(x,y)
# 나눗셈
x / y, torch.div(x,y)
# 내적 : x[0]*y[0] + x[1]*y[1] + x[2]*y[2]
# 합성곱층에서 커널을 통과시켜 특성을 추출할 때의 계산식과 동일
x.dot(y), torch.dot(x,y)
# 스칼라 연산
x + 1
# 행렬의 곱
x = torch.Tensor([[1,3], [2,4], [5,7]])
y = torch.Tensor([[7,9], [10,2]])
x.mm(y), torch.mm(x,y)
```

7. **sum, mean**

```python
z = torch.Tensor([1,2,3,4,5,6,7,8])
z = z.view(2,2,2)
torch.sum(z, dim=0)
torch.sum(z, dim=1)
torch.mean(z, dim=0)
```

8. **max, min, argmax, argmin**

```python
torch.max(z), torch.min(z)
# 인덱스 출력됨
torch.argmax(z), torch.argmin(z)
torch.max(z, dim=0)
```

9. **논리연산**

```python
z = torch.Tensor([-4,3,0])
print(z.eq(0))  # 같다
print(z.ge(0))  # 크거나 같다
print(z.le(0))  # 작거나 같다
print(z.gt(0))  # 크다
print(z.lt(0))  # 작다
```

# GPU 사용

## PyTorch GPU 사용

- colab에서 수정 > 노트 설정을 GPU 사용으로 변경
  - 런타임이 모두 초기화 됨

```python
# NDIVIA
device = 'cuda'
try:
  x = x.to(device)
  print(f'{device} 지원')
except Exception as e:
  print(f'{device} 미지원, {e}')

x.device
# 이 Tensor는 GPU에서 연산 -> Tensor별로 GPU, CPU 지정 가능
```



## TensorFlow GPU 사용

```python
import tensorflow as tf
from tensorflow.python.client import device_lib

# 지원되는 하드웨어 목록 중 device_type이 GPU인 것의 name: "/device:GPU:0" 이름 챙겨둠
# memory_limit 고려해 batch_size 조정
device_lib.list_local_devices()

# GPU 연산
with tf.device('/GPU:0'):
  a = tf.constant([ [1,2,3], [4,5,6] ])
  b = tf.constant([ [1,2], [3,4] ,[5,6] ])
  c = tf.matmul(a,b)
  print(c)
```



## Keras GPU 사용

```python
from keras import backend as K
K.tensorflow_backend._get_available_gpus()
sess = tf.Session()
from keras.backend import tensorflow_backend as k
# GPU의 메모리를 일부만 쓰다가 부족하면 자동으로 증가
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
K.set_session(tf.Session(config=config))
with tf.device('/GPU:0'):
  pass
sess.close()
```