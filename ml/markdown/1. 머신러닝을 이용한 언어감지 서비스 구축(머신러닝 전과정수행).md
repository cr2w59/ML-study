## 머신러닝을 이용한여 언어 감지 서비스 구축

### 1.  연구 목표 설정

- 유사서비스 : 파파고, 구글 번역
- 개요
    - 번역 서비스중 언어 감지 파트는 머신러닝의 지도학습법 중 분류를 사용하겟다
    - 알파벳을 사용하는 영어권에서는 알파벳 언어별로 알파벳의 사용 빈도가 다르다
- 조건
    - 비 영어권은 개별 방법론(완성형(utf-8), 조합형(euc-kr) 코드를 이용하여 판단) 배제
    
    - 임시값(100byte) 이내 문자열을 배제, 임시값의 임계값은 변경될수 있다
    
    - 번역서비스는 딥러닝의 RNN을 활용하여 처리하는데 여기서는 배제,단, 파파고 API를 활용하여 유사하게 구현
    
    - 서비스가 오픈하고 데이터가 축적되면 모델을 갱신(언어는 진화하니까) 모델을 다시 학습하고 교체를 진행하는데 원활하게 수행되겠금 처리(전략). 일단 여기서는 데이터 축적
    
      

|  No  | 단계                       | 내용                                                         |
| :--: | :------------------------- | :----------------------------------------------------------- |
|  1   | 연구 목표 설정             | - 웹서비스<br>- 사용자가 입력한 텍스트를 예측하여 어떤 언어인지 판독한다(영어권,알파벳 사용국가)<br>- 머신러닝의 지도학습-분류를 사용하겠다<br>- 파이프라인구축, 하이터파라미터튜닝을 이용한 최적화 부분은 제외<br>- 정량적인 목표치는 생략(평가배제)<br>- 임시값(100byte) 이내 문자열을 배제<br>- 논문을 통한 주장의 근거를 체크 |
|  2   | 데이터 획득/수집           | - 실전: 다양한 텍스트를 수집, 위키피디아, 법률, 소설등등<br>- 구현: 제공데이터를 사용(법령/대본/소설등) |
|  3   | 데이터 준비/통찰/전처리    | - 알파벳을 제외한 모든 문자 제거(전처리,정규식)<br>- 텍스트를 알파벳의 출현 빈도로 계산한다(문자계산, 데이터의 수치화)<br>- 데이터는 훈련 데이터(훈련(50), 검증(25))와 테스트 데이터(25)로 나눈다 (훈련:테스트=75:25) 황금비율, 단 바뀔수 있다 |
|  4   | 데이터 탐색/통찰/시각화    | - 논문의 주장을 증명<br>- 영어권 언어별로 알파벳 출현 빈도가 다르다는 명제를 증명/확인<br>- EDA 분석(시각화)를 이용하여 확인, 선형차트, 바차트 등을 활용 |
|  5   | 데이터 모델링 및 모델 구축 | - 알고리즘 선정<br>- 학습데이터/테스트데이터 준비<br>- 학습<br>- 예측<br>- 성능평가(학습법,하위 카테고리 까지 검토 평가)<br>- 파이프라인구축을 통하여 알고리즘 체인을 적용, 최적의 알고리즘 조합을 찾는다<br>- 연구 목표에 도착할때까지 반복 |
|  6   | 시스템 통합                | - 모델 덤프(학습된 알고리즘을 파일로 덤프)<br>- 웹서비스 구축(falsk 간단하게 구성)<br>- 서비스 구축<br>- 모델의 업그레이드를 위한 시스템 추가<br>- 선순화구조를 위한 번역 요청 데이터의 로그 처리->배치학습, 온라인 학습등으로 연결되어 완성 |



### 2.  데이터 획득

- 실전: 다양한 텍스트를 수집, 위키피디아, 법률, 소설등등<br>

  - 라이브러리: request, bs4<br>

  - 사이트: https://언어코드.wikipedia.org/wiki/검색어

- 구현: 제공데이터를 사용(법령/대본/소설등)

```python
import urllib.request as req
from bs4 import BeautifulSoup

# 데이터 획득 사이트 규격화
na_code = 'en'
keyword = 'Bong_Joon-ho'
target_site = f'https://{na_code}.wikipedia.org/wiki/{keyword}'

# 요청 및 SOUP 생성(DOM 트리)
# html5lib : 대량의 html 파싱을 위해 안정성 고려
soup = BeautifulSoup( req.urlopen(target_site), 'html5lib')

# 데이터 추출
texts = [ p.text for p in soup.select('#mw-content-text p') ]
text = ' '.join(texts)
```



### 3.  데이터 준비

- 알파벳을 제외한 모든 문자 제거(전처리,정규식)<br>
- 텍스트를 알파벳의 출현 빈도로 계산한다(문자계산, 데이터의 수치화)<br>
- 데이터는 훈련 데이터(훈련(50), 검증(25))와 테스트 데이터(25)로 나눈다 (훈련:테스트=75:25) 황금비율, 단 바뀔수 있다

```python

import os, glob, re
# glob: 패턴을 부여해서 특정 위치의 파일 목록을 가져옴

p = re.compile('[^a-zA-Z]')
target_string = p.sub('',text).lower()

# 파일 목록
file_list = glob.glob('../data/train/*.txt')

# 파일을 읽어서 알파벳 빈도 계산, 어떤 언어인지 데이터화
# 1. 파일경로 획득
pathName = file_list[0]
# 2. 파일명 획득
fName = os.path.basename(pathName)
# 3. 파일명에 정답(레이블)이 있음. 확장성을 고려해 정규식으로 추출
p2 = re.compile('^[a-z]{2}')
if p2.match(fName):
    lang = p2.match(fName).group()
# 4. 알파벳 빈도 계산
# 4-1 파일을 읽기
f = open(pathName,'r', encoding='utf-8')
data = f.read()
# 4-2 알파벳만 남겨서 소문자로 처리
data = p.sub('',data).lower()
# 4-3 알파벳별로 카운트 => 효율적인 로직 필요
wordcount = dict()
for j in data : 
    wordcount[j] = wordcount.get(j,0)+1
f.close()
```



