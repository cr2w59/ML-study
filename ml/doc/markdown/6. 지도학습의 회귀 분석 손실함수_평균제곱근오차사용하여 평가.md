### 참고
- 넷플릭스
    - 슬로건: 모든 것이 추천
    - https://medium.com/@NetflixTechBlog/ : 넷플릭스 추천 알고리즘 구현에 대한 기술 블로그 공개

## 1. 연구 목표
- 사용자 평점 데이터를 기반으로 사용자를 특정, 예측해 추천 시스템 구축
- 회귀 처리, 회귀 평가, 추천시스템에 대한 이해
- FastFM(third party 알고리즘 사용: 인수분해머신 기능 지원)
  - 윈도우에서는 컴파일 후 설치 불가/ 리눅스에서 설치해 진행



## 2. 데이터 수집/확보

- ml-100k.zip 파일
- 영화 정보 데이터(고객 정보/영화 정보/평점 정보)

```python
import pandas as pd
# 고객 정보
users = pd.read_csv('../table/ml-100k/u.user', sep='|', header=None)
users.columns = ['uid','age','m','job','zip_code']

# 영화 정보
cols = ['mid','title','release_date','video_release_date','imdb_url']
movies = pd.read_csv('../table/ml-100k/u.item', sep='|', header=None, encoding='latin1', names=cols, usecols=range(5))

# 평점 정보
ratings = pd.read_csv('../table/ml-100k/u.data', sep='\t', header=None, names=['uid','mid','rating','unix_timestamp'])
ratings['date'] = pd.to_datetime(ratings.unix_timestamp, unit='s')
```



## 3. 데이터 준비/품질향상/전처리

```python
movies_ratings = pd.merge(movies, ratings, on='mid')
movies_lens = pd.merge(movies_ratings, users)
movies_lens.title.value_counts()[:10]
```

- 데이터 병합 완료. 평점이나 회원을 중심으로 중복 데이터 많음
- 데이터가 크면 메모리를 많이 사용할 수도 있음



## 4. 데이터 분석(통계적,시각적)

```python
import numpy as np
# groupby(컬럼): 해당 컬림이 인덱스로 이동
# agg( {컬럼:[값처리함수]} ): 컬럼에 처리함수 개수대로 설정돼 값 자동 처리
movie_state=movies_lens.groupby('title').agg({'rating':[np.size, np.mean]})
```

- 평점을 받은 개수가 1개인 경우 평균에 잡음의 개입 여지 많음
- 임계값 100개

```python
limit_std_value = 100
condition = movie_state['rating']['size'] >= limit_std_value
# boolean indexing
movie_state[condition]
# 정렬: 컬럼 레벨이 1차를 넘는 경우 tuple로 지정
tmp = movie_state[condition].sort_values(by=[('rating','mean')], ascending=False)
```

```python
# 간단한 시각화
# x축: 평점 개수 / y축: 평가
from matplotlib import pyplot as plt
plt.style.use('ggplot')
movies_lens.groupby('uid').size().sort_values(ascending=False).hist()
# 빈도가 점점 낮아짐: 롱테일분포
# '지프의 법칙'을 따른 굴곡 모양
```



## 5. 예측모델 구축(머신러닝기반)

- 알고리즘 -> 인수분해 머신 기능을 제공하는 FastFM이라는 모듈 사용
- fastFM
    - c++로 만들어진 libFM이라는 알고리즘
    - libFM을 python으로 구성한 것이 fastFM
    - 기능 :
        - 행렬 인수 분해 일반화: 차원 축소
        - 범주형 변수를 파생변수로 변환하여 범주간의 상호 작용성을 계산
        - 특징간 영향을 주고받는 상호작용 개념을 계산

- fastFM 제공 알고리즘
    - ALS: 교대 최소 제곱법
        - 장점: 예측 시간이 빠름, SGD에 비해 하이퍼파라미터가 적다
        - 단점: 규제 처리(통제) 필요
    - SGD: 확률적 경사 하강법
        - 장점: 예측 시간이 빠름, 빅데이터를 빠르게 처리 학습 가능
        - 단점: 하이퍼파라미터가 많다, 규제 처리(통제) 필요 
    - MCMC: 마르코프 연쇄 몬테카를로
        - 장점: 하이퍼파라미터가 적다
        - 단점: 학습 시간이 느리다

```shell
# 아마존 EC2 서버에 접속
# febric3 이용하여 이하 과정 자동화 가능
# 리눅스 상에서 루트 권한 획득
ubuntu:$ sudo su
# 리눅스의 현재 설치된 패키지들을 최신으로 업그레이드
root:$ apt-get update && apt-get -y upgrade

# 패키지 설치
apt-get -y install python3-dev python3-pip git nano wget unzip libopenblas-dev

# fastFM 소스
git clone --recursive https://github.com/ibayer/fastFM.git
cd fastFM

# 필요한 모듈 내용 확인
cat requirements.txt
# 컴파일 수행 전 python 모듈 설치
pip3 install -r ./requirements.txt

# 컴파일->마지막 부분에 error가 보여도 무시
PYTHON=python3 make

# fastFM 설치
pip3 install .

# 확인
cat setup.py

python
>>> from fastFM import als
>>> exit()

# 개발에 필요한 패키지 설치
pip3 install pandas matplotlib jupyter

# root 계정 off
exit

# 작업 폴더 생성
cd ..
mkdir dev && cd dev

# jupyter 가동
jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root --no-browser

# http://13.125.248.156:8888/?token=52d1f813f99b1fec77f608eb955911aae329ba2e6a82bca5
```

<a href='http://13.125.248.156:8888'>리눅스 주피터 이동</a>

### 가상 데이터를 이용하여 기능 확인

```python
from sklearn.feature_extraction import DictVectorizer
import numpy as np

# DictVectorizer: 문자열만 벡터화 처리
v = DictVectorizer()

# 더미 데이터
# 사용자 ID, 사용자가 평가한 영화 ID, 사용자의 나이
train = [
    {'uid':'1', 'mid':'5', 'age':19},
    {'uid':'2', 'mid':'43', 'age':33},
    {'uid':'3', 'mid':'20', 'age':55},
    {'uid':'4', 'mid':'10', 'age':20},
]
X = v.fit_transform(train)
# 수치는 그대로 배치, 문자열은 벡터화(범주형으로 간주해 케이스별로 0/1 표시)
X.toarray()

from fastFM import als
from sklearn.model_selection import learning_curve

# user별로 부여한 평점
y = np.array( [5.0, 1.0, 2.0, 4.0] )
# ALS를 이용해 fastFM의 회귀모델을 초기화 후 학습 진행
# 하이퍼파라미터는 임시값 부여
fm = als.FMRegression( n_iter=1000, init_stdev=0.1, rank=2, l2_reg_w=0.1, l2_reg_V=0.5)
fm.fit(X, y)

# 예측
# ex. 24살인 user가 10번 영화에 대해 부여할 평점 예측하기
fm.predict( v.transform( {'uid':'5', 'mid':'10', 'age':24} ) )
```

- 제공되는 데이터 ua.base, ua.test는 훈련용과 테스트용으로 구분
- 데이터를 자료구조로 처리하는 함수 구현

```python
# 평가에 참여한 유저들의 ID만 모은 데이터셋, 영화의 ID만 모은 데이터셋
def loadData(fileName, path='../table/ml-100k/'):
    data = list() # 학습용
    y = list() # 평점
    # 중복 데이터를 제거하는 자료구조 생성
    users = set()
    movies = set()
    with open(path + fileName) as f:
        for line in f:
            uid, mid, rating, ts = line.split('\t')
            data.append({'uid':str(uid), 'mid':str(mid)})
            y.append(float(rating))
            users.add(uid)
            movies.add(mid)
    return data, np.array(y), users, movies

# 훈련용 데이터 획득
dev_data, dev_y, dev_users, dev_movies = loadData('ua.base')
# 테스트용 데이터 획득
test_data, test_y, test_users, test_movies = loadData('ua.test')

```

- 데이터의 벡터화 - 평점을 제외한 uid, mid는 문자열
- 훈련용 데이터의 벡터화

```python
# 데이터의 벡터화
v = DictVectorizer()
X_dev = v.fit_transform(dev_data)
X_test = v.fit_transform(test_data)

# 표준편차 확인: 회귀에서 평가지수 -> 평균 제곱근 오차 계산 시 평가의 잣대
np.std(dev_y), np.std(test_y)

from sklearn.model_selection import train_test_split
X_train, X_dev_test, y_train, y_dev_test = train_test_split(
    									X_dev, dev_y, test_size=0.1, random_state=59)
```

- 알고리즘 선택
    - mcmc
        - 학습 및 예측 수행
        - 시각화를 통해 수렴해나가는 과정
        - 테스트데이터를 이용한 성능 측정
            - 평균제곱근오차 및 손실함수 이용해 평가
        - 하이퍼파라미터 활용해 평점의 정규화 처리, 성능 평가

```python
# 평균제곱근오차함수
from sklearn.metrics import mean_squared_error
from fastFM import mcmc

#0. 실험수치 설정
# mcmc의 하이퍼파라미터 및 실험 임계값 변수 지정
rank = 4
seed = 123
# 실험 임계값
n_iter = 300
step_size = 1 # 훈련시 사용되는 값(사용할때 확인)

# 1.알고리즘생성
# n_iter : 샘플수 (als에서는 학습세트의 반복수 sgd에서는 단계)
# rank=인수분해의 순위
fm = mcmc.FMRegression(n_iter=0, rank=rank, random_state=seed)

# 2. 데이터 준비(완료)

# 3. 훈련 및 예측
fm.fit_predict(X_train, y_train, X_dev_test)

# 4. 손실함수(:정답에 가까운 오류를 수치로 표현/정답에 가까울수록 값이 작아짐) 어떤 값으로 수렴하는지 확인
# 회귀모델에서 성능평가를 처리하는 지표 중 하나(평균제곱근오차를 통한 손실함수, 결정계수)
# 평균제곱근오차함수 값을 제곱근 처리
# 평균제곱근오차함수의 인자값(예측값, 예측에 사용한 데이터의 답)
# 2차원 0 행렬
hyper_param = np.zeros( (n_iter-1, rank * 2 + 3), dtype=np.float64 )

# 구현 환경은 n_iter만큼 반복 수행
rmse_test = list()
for nr, i in enumerate(range(1, n_iter)):
    # 난수 변경 (훈련/검증 폴드 성분 변경)
    fm.random_state = i * seed
    # 학습 및 예측
    y_pred = fm.fit_predict(X_train, y_train, X_dev_test, n_more_iter=step_size)
    # 손실함수 값 획득
    loss_v = np.sqrt(mean_squared_error(y_pred, y_dev_test))
    rmse_test.append(loss_v)
    
    # fm.hyper_param_: 배열, 11개의 성분 가짐
#     print(fm.hyper_param_, type(fm.hyper_param_))
    
    hyper_param[nr,:] = fm.hyper_param_
    
x = np.arange(1, n_iter)

# 시각화를 통해 값의 수렴 추이 확인
from matplotlib import pyplot as plt
fig, axes = plt.subplots( nrows=2, ncols=2, sharex=True, figsize=(15,8) )
# x축은 1~299, 학습 수행 횟수
axes[0,0].plot( x, rmse_test, label='dev test rmse', color='r' )
axes[0,1].plot( x, hyper_param[:, 0], label='alpha', color='g' )
axes[1,0].plot( x, hyper_param[:, 1], label='lambda_w', color='b' )
axes[1,1].plot( x, hyper_param[:, 3], label='mu_w', color='y' )
axes[0,0].legend()
axes[0,1].legend()
axes[1,0].legend()
axes[1,1].legend()
```

![](C:\Users\admin\Documents\GitHub\pengsoo\ml\table\회귀_시각화_1.png)

```python
# 초기는 성능이 안 나오는 지점이기 때문에 그래프가 길게 보이고 세부 변화는 보이지 않는 문제가 있어서 더욱 상세하게 표현
# 상위 5개 데이터는 누락해서 처리(lambda_w의 값이 10 정도에서 수렴하는 것으로 보이기 때문)
fig, axes = plt.subplots( nrows=2, ncols=2, sharex=True, figsize=(15,8) )
# x축은 1~299
skip_idx = 5
axes[0,0].plot( x[skip_idx:], rmse_test[skip_idx:], label='dev test rmse', color='r' )
axes[0,1].plot( x[skip_idx:], hyper_param[skip_idx:, 0], label='alpha', color='g' )
axes[1,0].plot( x[skip_idx:], hyper_param[skip_idx:, 1], label='lambda_w', color='b' )
axes[1,1].plot( x[skip_idx:], hyper_param[skip_idx:, 3], label='mu_w', color='y' )

axes[0,0].legend()
axes[0,1].legend()
axes[1,0].legend()
axes[1,1].legend()
```

![](C:\Users\admin\Documents\GitHub\pengsoo\ml\table\회귀_시각화_2.png)

```python
# 손실함수의 최소값: 예측 수행 후 획득한 값->최초 기준값
# 이 수치를 낮추는 게 성능을 향상시키는 것
np.min(rmse_test)
np.std(y_dev_test)
```



- 반복횟수가 100~150번 사이로 진입하면 rmse 값은 안정화(수렴)
- 하이퍼파라미터들은 약 100회, 150회 지점에서 안정화
- 검증, 테스트 데이터 등 표준편차가 회귀 문제의 손실함수 값의 평가를 담당
    - 경향
        - 평균 제곱근 오차 값의 제곱근(손실함수)은 표준편차에 가까워지려는 경향이 있다
        - 성능 향상은 반드시 값을 줄이는 방향이 아닌, 표준편차에 가까워지게 하는 방향으로 전개

```python
ranks = [4, 8, 16, 32, 64]
seed = 333
n_iter = 100
rmse_test = list()
for rank in ranks:
    # rank만 바꿔가면서 rmse에 미치는 영향 확인
    fm = mcmc.FMRegression(n_iter=n_iter, rank=rank, random_state=seed)
    y_pred = fm.fit_predict(X_train, y_train, X_dev_test)
    loss_v = np.sqrt(mean_squared_error(y_pred, y_dev_test))
    rmse_test.append(loss_v)
    print('rank:{}\trmse:{:.3f}'.format(rank, loss_v))
```

- rank 32부터 값이 수렴

```python
# 시각화
plt.plot(ranks, rmse_test, label='rank per rmse', color='r')
plt.legend()
print( np.min(rmse_test) )
```

![](C:\Users\admin\Documents\GitHub\pengsoo\ml\table\회귀_시각화_3.png)

```python
# 훈련, 검증에 한번도 관여하지 않은 데이터 X_test, y_test를 rank=32로 평가
fm = mcmc.FMRegression(n_iter=300, rank=32, random_state=seed)
y_pred = fm.fit_predict(X_train, y_train, X_test)
np.sqrt(mean_squared_error(y_pred, test_y))
```

- 상위에서 획득한 값에 비해서 높게 나옴
- 하이퍼파라미터는 고정하고 최적화를 생각해볼때 편차가 크다고 느낄 수 있는 파트인 y_train (1~5)을 0~1로 정규화 필요

```python
from sklearn.preprocessing import StandardScaler
# 평점 정규화를 위해서 standardScaler를 사용
scaler = StandardScaler()
# y_train을 2차원 배열로 재구성
y_train_norm = scaler.fit_transform(y_train.reshape(-1,1)).ravel()

fm = mcmc.FMRegression(n_iter=300, rank=32, random_state=seed)
y_pred = fm.fit_predict(X_train, y_train_norm, X_test)
np.sqrt(mean_squared_error(y_pred, scaler.transform(test_y.reshape(-1,1))))

# 하이퍼파라미터 고정, 평점 정규화 후 수렴 과정 확인
ranks = 32
seed = 123
n_iter = 200
step_size = 1

fm = mcmc.FMRegression(n_iter=0, rank=rank, random_state=seed)
y_pred = fm.fit_predict(X_train, y_train_norm, X_test)
rmse_test = list()
hyper_param = np.zeros( (n_iter-1, 2*rank+3), dtype=np.float64)
for nr, i in enumerate(range(1, n_iter)):
    fm.random_state = i * seed
    y_pred = fm.fit_predict(X_train, y_train_norm, X_test, n_more_iter=step_size)
    # 예측값을 되돌려서 y_test 원래값(1~5)으로 돌아오게
    rmse_test.append( np.sqrt( mean_squared_error( scaler.inverse_transform(y_pred), test_y ) ) )
    hyper_param[nr,:] = fm.hyper_param_
    
# 시각화
x = np.arange(1, n_iter) * step_size

fig, axes = plt.subplots( nrows=2, ncols=2, sharex=True, figsize=(15,8) )

skip_idx = 5
axes[0,0].plot( x[skip_idx:], rmse_test[skip_idx:], label='dev test rmse', color='r' )
axes[0,1].plot( x[skip_idx:], hyper_param[skip_idx:, 0], label='alpha', color='g' )
axes[1,0].plot( x[skip_idx:], hyper_param[skip_idx:, 1], label='lambda_w', color='b' )
axes[1,1].plot( x[skip_idx:], hyper_param[skip_idx:, 3], label='mu_w', color='y' )

axes[0,0].legend()
axes[0,1].legend()
axes[1,0].legend()
axes[1,1].legend()

```

![](C:\Users\admin\Documents\GitHub\pengsoo\ml\table\회귀_시각화_4.png)

```python
# 최저값
np.min(rmse_test)
# y가 최저값일 때의 x값
x[np.argmin(rmse_test)]
```

#### 정보추가
- 현재는 사용자 ID, 영화 ID만 존재
- 개봉연도, 사용자 나이, 성별 등 추가해서 특징들을 조합

```python
movies_lens['date1'] = movies_lens['date'].apply( lambda x : str(x.year) )
# 데이터 추가를 위해 조정
movies_lens['uid'] = movies_lens['uid'].astype(str)
movies_lens['mid'] = movies_lens['mid'].astype(str)
# 컬럼 조합 후보 리스트
candidate_columns = [
    ['uid','mid','age','m','date1','rating'],
    ['uid','mid','m','date1','rating'],
    ['uid','mid','age','m','rating'],
    ['uid','mid','rating'],
]

# 실험 임계값
rmse_test = list()
n_iter = 500
seed = 123
rank = 8
split_seed = 42

# 조합별 반복 훈련 및 검증
for column in candidate_columns:
    # 누락값 제거
    filterNaN_lens = movies_lens[column].dropna()
    # 벡터화(평점 부분 고려)
    v = DictVectorizer()
    tmp = list( filterNaN_lens.drop('rating', axis=1).T.to_dict().values() )
    X_feature = v.fit_transform(tmp)
    # 정답
    y_feature = filterNaN_lens['rating'].tolist()
    
    # 데이터 분류(훈련/테스트)
    X_mc_train, X_mc_test, y_mc_train, y_mc_test = train_test_split(X_feature, y_feature, test_size=0.1, random_state=split_seed)
    
    # 평점 정규화
    scaler = StandardScaler()
    y_mc_train_norm = scaler.fit_transform(np.array(y_mc_train).reshape(-1,1)).ravel()
    
    # 알고리즘 생성
    fm = mcmc.FMRegression(n_iter=n_iter, rank=rank, random_state=seed)
    
    # 훈련 및 예측
    y_pred = fm.fit_predict(X_mc_train, y_mc_train_norm, X_mc_test)
    
    # rmse값 계산
    tmp2 = scaler.inverse_transform(y_pred.reshape(-1,1))
    rmse_test.append( np.sqrt(mean_squared_error(tmp2, y_mc_test)) )
```

```python
# 시각화
x_incre = np.arange( len(candidate_columns) )
bar = plt.bar(x_incre, height=rmse_test )
plt.xticks(x_incre, list('ABCD'))
plt.ylim(0.88, 0.9)
```

![](C:\Users\admin\Documents\GitHub\pengsoo\ml\table\회귀_시각화_5.png)

- 추천 시스템 알고리즘
    - 협업 필터링: collaborative filtering
        - 메모리 기반 협업 필터링
            - 사용자 기반
            - 아이템 기반
        - 모델 기반 협업 필터링
    - 내용 기반 필터링



- 사용자 기반
    : 특정 고객과 비슷한 상품을 산 다른 고객은 이런 상품도 샀다
    : 피어슨 상관관계, 코사인 유도, 자카드 계수
- 아이템 기반 협업 필터링
    : 새로 가입한 사용자, 서비스 초기 시도하기 좋음. 인기 아이템만 노출시켜 의외성을 보인 부분 보강
    : 개선된 코사인 유도
- 모델 기반 협업 필터링
    : 지도, 비지도 학습법으로 모델을 학습시키고, 기존 데이터가 가진 규칙성에 따라 예측
    - 방법
        1. 군집화를 통한 모델링
        2. 평점을 통한 회귀 모델
        3. 토픽 모델을 사용한 방법
        4. 행렬 분해방식
- 내용 기반 협업 필터링
    : 영화의 제목, 주인공, 감독, 장르, 배우, 평판 등의 아이템을 나타내는 정보에 주목

## 6. 시스템 통합(서비스에 반영 ex. OTT, 쇼핑몰)

