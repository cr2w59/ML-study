### 참고
- 넷플릭스
    - 슬로건: 모든 것이 추천
    - https://medium.com/@NetflixTechBlog/ : 넷플릭스 추천 알고리즘 구현에 대한 기술 블로그 공개

## 1. 연구 목표
- 사용자 평점 데이터를 기반으로 사용자를 특정, 예측해 추천 시스템 구축
- 회귀 처리, 회귀 평가, 추천시스템에 대한 이해
- FastFM(third party 알고리즘 사용: 인수분해머신 기능 지원)
  - 윈도우에서는 컴파일 후 설치 불가/ 리눅스에서 설치해 진행



## 2. 데이터 수집/확보

- ml-100k.zip 파일
- 영화 정보 데이터(고객 정보/영화 정보/평점 정보)

```python
import pandas as pd
# 고객 정보
users = pd.read_csv('../table/ml-100k/u.user', sep='|', header=None)
users.columns = ['uid','age','m','job','zip_code']

# 영화 정보
cols = ['mid','title','release_date','video_release_date','imdb_url']
movies = pd.read_csv('../table/ml-100k/u.item', sep='|', header=None, encoding='latin1', names=cols, usecols=range(5))

# 평점 정보
ratings = pd.read_csv('../table/ml-100k/u.data', sep='\t', header=None, names=['uid','mid','rating','unix_timestamp'])
ratings['date'] = pd.to_datetime(ratings.unix_timestamp, unit='s')
```



## 3. 데이터 준비/품질향상/전처리

```python
movies_ratings = pd.merge(movies, ratings, on='mid')
movies_lens = pd.merge(movies_ratings, users)
movies_lens.title.value_counts()[:10]
```

- 데이터 병합 완료. 평점이나 회원을 중심으로 중복 데이터 많음
- 데이터가 크면 메모리를 많이 사용할 수도 있음



## 4. 데이터 분석(통계적,시각적)

```python
import numpy as np
# groupby(컬럼): 해당 컬림이 인덱스로 이동
# agg( {컬럼:[값처리함수]} ): 컬럼에 처리함수 개수대로 설정돼 값 자동 처리
movie_state=movies_lens.groupby('title').agg({'rating':[np.size, np.mean]})
```

- 평점을 받은 개수가 1개인 경우 평균에 잡음의 개입 여지 많음
- 임계값 100개

```python
limit_std_value = 100
condition = movie_state['rating']['size'] >= limit_std_value
# boolean indexing
movie_state[condition]
# 정렬: 컬럼 레벨이 1차를 넘는 경우 tuple로 지정
tmp = movie_state[condition].sort_values(by=[('rating','mean')], ascending=False)
```

```python
# 간단한 시각화
# x축: 평점 개수 / y축: 평가
from matplotlib import pyplot as plt
plt.style.use('ggplot')
movies_lens.groupby('uid').size().sort_values(ascending=False).hist()
# 빈도가 점점 낮아짐: 롱테일분포
# '지프의 법칙'을 따른 굴곡 모양
```



## 5. 예측모델 구축(머신러닝기반)

- 알고리즘 -> 인수분해 머신 기능을 제공하는 FastFM이라는 모듈 사용
- fastFM
    - c++로 만들어진 libFM이라는 알고리즘
    - libFM을 python으로 구성한 것이 fastFM
    - 기능 :
        - 행렬 인수 분해 일반화: 차원 축소
        - 범주형 변수를 파생변수로 변환하여 범주간의 상호 작용성을 계산
        - 특징간 영향을 주고받는 상호작용 개념을 계산

- fastFM 제공 알고리즘
    - ALS: 교대 최소 제곱법
        - 장점: 예측 시간이 빠름, SGD에 비해 하이퍼파라미터가 적다
        - 단점: 규제 처리(통제) 필요
    - SGD: 확률적 경사 하강법
        - 장점: 예측 시간이 빠름, 빅데이터를 빠르게 처리 학습 가능
        - 단점: 하이퍼파라미터가 많다, 규제 처리(통제) 필요 
    - MCMC: 마르코프 연쇄 몬테카를로
        - 장점: 하이퍼파라미터가 적다
        - 단점: 학습 시간이 느리다

```shell
# 아마존 EC2 서버에 접속
# febric3 이용하여 이하 과정 자동화 가능
# 리눅스 상에서 루트 권한 획득
ubuntu:$ sudo su
# 리눅스의 현재 설치된 패키지들을 최신으로 업그레이드
root:$ apt-get update && apt-get -y upgrade

# 패키지 설치
apt-get -y install python3-dev python3-pip git nano wget unzip libopenblas-dev

# fastFM 소스
git clone --recursive https://github.com/ibayer/fastFM.git
cd fastFM

# 필요한 모듈 내용 확인
cat requirements.txt
# 컴파일 수행 전 python 모듈 설치
pip3 install -r ./requirements.txt

# 컴파일->마지막 부분에 error가 보여도 무시
PYTHON=python3 make

# fastFM 설치
pip3 install .

# 확인
cat setup.py

python
>>> from fastFM import als
>>> exit()

# 개발에 필요한 패키지 설치
pip3 install pandas matplotlib jupyter

# root 계정 off
exit

# 작업 폴더 생성
cd ..
mkdir dev && cd dev

# jupyter 가동
jupyter notebook --ip=0.0.0.0 --port=8888 --allow-root --no-browser

# http://13.125.248.156:8888/?token=52d1f813f99b1fec77f608eb955911aae329ba2e6a82bca5
```

<a href='http://13.125.248.156:8888'>리눅스 주피터 이동</a>

### 가상 데이터를 이용하여 기능 확인

```python
from sklearn.feature_extraction import DictVectorizer
import numpy as np

# DictVectorizer: 문자열만 벡터화 처리
v = DictVectorizer()

# 더미 데이터
# 사용자 ID, 사용자가 평가한 영화 ID, 사용자의 나이
train = [
    {'uid':'1', 'mid':'5', 'age':19},
    {'uid':'2', 'mid':'43', 'age':33},
    {'uid':'3', 'mid':'20', 'age':55},
    {'uid':'4', 'mid':'10', 'age':20},
]
X = v.fit_transform(train)
# 수치는 그대로 배치, 문자열은 벡터화(범주형으로 간주해 케이스별로 0/1 표시)
X.toarray()

from fastFM import als
from sklearn.model_selection import learning_curve

# user별로 부여한 평점
y = np.array( [5.0, 1.0, 2.0, 4.0] )
# ALS를 이용해 fastFM의 회귀모델을 초기화 후 학습 진행
# 하이퍼파라미터는 임시값 부여
fm = als.FMRegression( n_iter=1000, init_stdev=0.1, rank=2, l2_reg_w=0.1, l2_reg_V=0.5)
fm.fit(X, y)

# 예측
# ex. 24살인 user가 10번 영화에 대해 부여할 평점 예측하기
fm.predict( v.transform( {'uid':'5', 'mid':'10', 'age':24} ) )
```

- 제공되는 데이터 ua.base, ua.test는 훈련용과 테스트용으로 구분
- 데이터를 자료구조로 처리하는 함수 구현

```python
# 평가에 참여한 유저들의 ID만 모은 데이터셋, 영화의 ID만 모은 데이터셋
def loadData(fileName, path='../table/ml-100k/'):
    data = list() # 학습용
    y = list() # 평점
    # 중복 데이터를 제거하는 자료구조 생성
    users = set()
    movies = set()
    with open(path + fileName) as f:
        for line in f:
            uid, mid, rating, ts = line.split('\t')
            data.append({'uid':str(uid), 'mid':str(mid)})
            y.append(float(rating))
            users.add(uid)
            movies.add(mid)
    return data, np.array(y), users, movies

# 훈련용 데이터 획득
dev_data, dev_y, dev_users, dev_movies = loadData('ua.base')
# 테스트용 데이터 획득
test_data, test_y, test_users, test_movies = loadData('ua.test')

```

- 데이터의 벡터화 - 평점을 제외한 uid, mid는 문자열
- 훈련용 데이터의 벡터화

```python
# 데이터의 벡터화
X_dev = v.fit_transform(dev_data)
X_test = v.fit_transform(test_data)

# 표준편차 확인: 회귀에서 평가지수 -> 평균 제곱근 오차 계산 시 평가의 잣대
np.std(dev_y), np.std(test_y)

from sklearn.model_selection import train_test_split
X_train, X_dev_test, y_train, y_dev_test = train_test_split(
    									X_dev, dev_y, test_size=0.1, random_state=59)
```

- 알고리즘 선택
    - mcmc
        - 학습 및 예측 수행
        - 시각화를 통해 수렴해나가는 과정
        - 테스트데이터를 이용한 성능 측정
            - 평균제곱근오차 및 손실함수 이용해 평가
        - 하이퍼파라미터 활용해 평점의 정규화 처리, 성능 평가



## 6. 시스템 통합(서비스에 반영 ex. OTT, 쇼핑몰)

