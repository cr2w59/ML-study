## 머신러닝을 이용한여 언어 감지 서비스 구축

### 1.  연구 목표 설정

- 유사서비스 : 파파고, 구글 번역
- 개요
    - 번역 서비스중 언어 감지 파트는 머신러닝의 지도학습법 중 분류를 사용하겟다
    - 알파벳을 사용하는 영어권에서는 알파벳 언어별로 알파벳의 사용 빈도가 다르다
- 조건
    - 비 영어권은 개별 방법론(완성형(utf-8), 조합형(euc-kr) 코드를 이용하여 판단) 배제
    
    - 임시값(100byte) 이내 문자열을 배제, 임시값의 임계값은 변경될수 있다
    
    - 번역서비스는 딥러닝의 RNN을 활용하여 처리하는데 여기서는 배제,단, 파파고 API를 활용하여 유사하게 구현
    
    - 서비스가 오픈하고 데이터가 축적되면 모델을 갱신(언어는 진화하니까) 모델을 다시 학습하고 교체를 진행하는데 원활하게 수행되겠금 처리(전략). 일단 여기서는 데이터 축적
    
      

|  No  | 단계                       | 내용                                                         |
| :--: | :------------------------- | :----------------------------------------------------------- |
|  1   | 연구 목표 설정             | - 웹서비스<br>- 사용자가 입력한 텍스트를 예측하여 어떤 언어인지 판독한다(영어권,알파벳 사용국가)<br>- 머신러닝의 지도학습-분류를 사용하겠다<br>- 파이프라인구축, 하이터파라미터튜닝을 이용한 최적화 부분은 제외<br>- 정량적인 목표치는 생략(평가배제)<br>- 임시값(100byte) 이내 문자열을 배제<br>- 논문을 통한 주장의 근거를 체크 |
|  2   | 데이터 획득/수집           | - 실전: 다양한 텍스트를 수집, 위키피디아, 법률, 소설등등<br>- 구현: 제공데이터를 사용(법령/대본/소설등) |
|  3   | 데이터 준비/통찰/전처리    | - 알파벳을 제외한 모든 문자 제거(전처리,정규식)<br>- 텍스트를 알파벳의 출현 빈도로 계산한다(문자계산, 데이터의 수치화)<br>- 데이터는 훈련 데이터(훈련(50), 검증(25))와 테스트 데이터(25)로 나눈다 (훈련:테스트=75:25) 황금비율, 단 바뀔수 있다 |
|  4   | 데이터 탐색/통찰/시각화    | - 논문의 주장을 증명<br>- 영어권 언어별로 알파벳 출현 빈도가 다르다는 명제를 증명/확인<br>- EDA 분석(시각화)를 이용하여 확인, 선형차트, 바차트 등을 활용 |
|  5   | 데이터 모델링 및 모델 구축 | - 알고리즘 선정<br>- 학습데이터/테스트데이터 준비<br>- 학습<br>- 예측<br>- 성능평가(학습법,하위 카테고리 까지 검토 평가)<br>- 파이프라인구축을 통하여 알고리즘 체인을 적용, 최적의 알고리즘 조합을 찾는다<br>- 연구 목표에 도착할때까지 반복<br/>- 모델 덤프(학습된 알고리즘을 파일로 덤프) |
|  6   | 시스템 통합                | - 웹서비스 구축(falsk 간단하게 구성)<br>- 서비스 구축<br>- 모델의 업그레이드를 위한 시스템 추가<br>- 선순환 구조를 위한 번역 요청 데이터의 로그 처리->배치학습, 온라인 학습등으로 연결되어 완성 |



### 2.  데이터 획득

- 실전: 다양한 텍스트를 수집, 위키피디아, 법률, 소설등등<br>

  - 라이브러리: request, bs4<br>
- 사이트: https://언어코드.wikipedia.org/wiki/검색어
            - 예) https://en.wikipedia.org/wiki/Bong_Joon-ho

- 구현: 제공데이터를 사용(법령/대본/소설등)

```python
import urllib.request as req
from bs4 import BeautifulSoup

# 데이터 획득 사이트 규격화
na_code = 'en'
keyword = 'Bong_Joon-ho'
target_site = f'https://{na_code}.wikipedia.org/wiki/{keyword}'

# 요청 및 SOUP 생성(DOM 트리)
# html5lib : 대량의 html 파싱을 위해 안정성 고려
soup = BeautifulSoup( req.urlopen(target_site), 'html5lib')

# 데이터 추출
texts = [ p.text for p in soup.select('#mw-content-text p') ]
text = ' '.join(texts)
```



### 3.  데이터 준비

- 알파벳을 제외한 모든 문자 제거(전처리,정규식)<br>
- 텍스트를 알파벳의 출현 빈도로 계산한다(문자계산, 데이터의 수치화)<br>
- 데이터는 훈련 데이터(훈련(50), 검증(25))와 테스트 데이터(25)로 나눈다 (훈련:테스트=75:25) 황금비율, 단 바뀔수 있다

```python
import os, glob, re, json
# glob: 패턴을 부여해서 특정 위치의 파일 목록을 가져옴

p = re.compile('[^a-zA-Z]')
target_string = p.sub('',text).lower()

# 파일 목록
file_list = glob.glob('../data/train/*.txt')

# 파일을 읽어서 알파벳 빈도 계산, 어떤 언어인지 데이터화
# 1. 파일경로 획득
pathName = file_list[0]
# 2. 파일명 획득
fName = os.path.basename(pathName)
# 3. 파일명에 정답(레이블)이 있음. 확장성을 고려해 정규식으로 추출
p2 = re.compile('^[a-z]{2}')
if p2.match(fName):
    lang = p2.match(fName).group()
# 4. 알파벳 빈도 계산
with open(pathName,'r', encoding='utf-8') as f:
    # 4-1 파일을 읽기
    # 4-2 알파벳만 남겨서 소문자로 처리
    text = f.read().lower()
    p3 = re.compile('[^a-z]')
    text = p3.sub('',text)
    pass

# 파일 경로를 넣으면 -> 정답(국가코드), 알파벳빈도를 계산한 데이터를 리턴하는 함수
def detect_trans_lang_freq( pathName ):
    # 파일명 획득
    fileName = os.path.basename( pathName )
    # 정답, 국가코드 획득
    if p2.match( fileName ):
        lang = p2.match( fileName ).group()
    else:
        return None, None
    # -----------------------------------
    with open( pathName, 'r', encoding='utf-8' ) as f:
        text = f.read().lower()
        text = p3.sub('', text)
    counts  = [ 0 for n in range(26) ] # 알파벳별 카운트를 담을 공간(리스트)
    limit_a = ord('a') # 매번 반복해서 작업하니까 그냥 최초 한번 변수로 받아서 사용
    for word in text:
        counts[ord(word)-limit_a] += 1 # 문자 1개당 카운트 추가 
    # 빈도수는 값이 너무 퍼져 있어서 0~1사이로 정규화를 하겠다=>학습효과가 뛰어나니까
    total_count = sum(counts)
    freq = list( map( lambda x:x/total_count , counts ) )        
    return lang, freq

# train을 입력으로 넣으면 훈련용 데이터를 싹다 뽑아온다, ( <-> test )
def load_data( style='train' ):
    langs = []
    freqs = []
    file_list = glob.glob(f'../data/{style}/*.txt')    
    for file in file_list:
        lang, freq = detect_trans_lang_freq(file)
        langs.append( lang )
        freqs.append( freq )
    # 딕셔너리 정적 구성으로 최종 데이터 형태를 맞춰준다 => Dataframe 형태 고려
    return { 'labels':langs, 'freqs':freqs }
# 훈련용
train_data = load_data()
# 테스트용
test_data  = load_data('test')
# 최종 준비된 데이터를 덤프해서 4단계쪽으로 제공
with open( '../data/labels_freqs.json', 'w', encoding='utf-8' ) as f:
    json.dump( [ train_data, test_data ], f )
```



### 4. 데이터 탐색

- 논문의 주장을 증명
- 영어권 언어별로 알파벳 출현 빈도가 다르다는 명제를 증명/확인
- EDA 분석(시각화)를 이용하여 확인, 선형차트, 바차트 등을 활용

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from string import ascii_lowercase

with open('../data/labels_freqs.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
# 훈련 데이터를 df로 변환
df_freqs = pd.DataFrame( data[0]['freqs'] )
df_labels = pd.DataFrame(data[0]['labels'])
df_tmp = pd.merge( df_freqs, df_labels, left_index=True, right_index=True )
df_tmp.columns = list(ascii_lowercase) + [df_tmp.columns[-1]]
# 피벗을 이용하여서 인덱스에 a~z, 컬럼 label으로 재구성
# 목적은 시각화를 용이하게 처리하기 위해서 관점 전환
df_tmp_pv = df_tmp.pivot_table( columns=df_tmp.label )

# EDA(시각화 분석)
# 바차트
plt.style.use('ggplot')
df_tmp_pv.plot( kind='bar', subplots=True, figsize=(16,8), ylim=(0, 0.23) )
plt.savefig('./data/plot_bar_langPerFrequences.png')

# 히스토그램을 이용하여 알파벳별 국가별 사용 빈도 비교 
# 피벗 데이터가 아닌 원데이터를 이용하여 표시 => df_tmp 사용
nas = df_tmp['label'].unique()
for word in ascii_lowercase:    
    for na in nas:
        # 해당 국가의 데이터만 일단 모은다
        df_tmp_na = df_tmp[ df_tmp['label'] == na ]
        # 영어의 a, 영어의 b, ....타갈리아어의 z
        df_tmp_na[word].plot( kind='hist', alpha=0.4, label=na )
    # 범례
    plt.legend()
    # 제목 : a freq, b freq, .... z freq
    plt.suptitle('%s freq' % word)
    # 저장
    plt.savefig('./data/%s_freq.png' % word)
    # 화면 보이기
    plt.show()
    
# 선형 그래프를 통해서 확인
df_tmp_pv.plot( kind='line', figsize=(16,8), ylim=(0, 0.23) )
plt.show()
```



### 5. 데이터 모델링 및 모델 구축

- 알고리즘 선정
- 학습데이터/테스트데이터 준비
- 학습
- 예측
- [생략]성능평가(학습법,하위 카테고리 까지 검토 평가)
- [생략]파이프라인구축을 통하여 알고리즘 체인을 적용, 최적의 알고리즘 조합을 찾는다
- [생략]연구 목표에 도착할때까지 반복

```python
from sklearn import svm, metrics
from sklearn.externals import joblib

# 1. 알고리즘 선정
clf = svm.SVC( gamma='auto' )

# 2. 학습데이터/테스트데이터 준비
# 75:25 = 훈련데이터(50:훈련용,25:검증용):테스트데이터 
# 이전단계에서 이미 구성이 완료되었다 (생략)

# 3. 학습
clf.fit( data[0]['freqs'], data[0]['labels'] )

# 4. 예측
# 훈련(학습)에 사용된 데이터는 사용 불가
predict = clf.predict( data[1]['freqs']  )

# 5. 결과 확인, 성능확인
# 정확도 87% 이상이면 실제 사용 가능하다고 간주하고(설정) 목표 도달로 간주
score = metrics.accuracy_score( data[1]['labels'], predict )

# 6. 레포트
cl_report = metrics.classification_report( data[1]['labels'], predict )
# precision    :  정밀도
# recall       :  재현율
# f1-score     :  점수, 정확도
# support      :  데이터에 응답한 샘플수

# 학습시킨 알고리즘을 덤프하여서 6단계 개발자에게 전달
# 사용 메뉴얼, 라이브러리(텍스트 => 빈도계산, 답안 처리하는 데이터 형태로 생성)
# 저장
joblib.dump(clf, '../data/clf_model_202001161419.model')
# 레이블 저장(분류에 해당되기 때문에 알고리즘과 답안 데이터 모두 저장)
label_dic ={
    'en':'영어',
    'fr':'프랑스어',
    'tl':'타갈리아어',
    'id':'인도네시아어'
}
# json 덤프
with open( '../data/clf_labels.label', 'w', encoding='utf-8' ) as f:
    json.dump( label_dic, f )
```



### 6. 시스템 통합

- 웹서비스 구축(falsk 간단하게 구성)
    - IDE : vs code를 사용 
- 서비스 구축
- 모델의 업그레이드를 위한 시스템 추가
- 선순환구조를 위한 번역 요청 데이터의 로그 처리->배치학습, 온라인 학습등으로 연결되어 완성